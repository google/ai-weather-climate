{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MVf2yC6x3k4D"
      },
      "source": [
        "Copyright 2022 Google LLC.\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\");"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "6_AEv20S3nYt"
      },
      "outputs": [],
      "source": [
        "#@title License\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-GYDMGwH3ppX"
      },
      "source": [
        "# MetNet-2 Model Skeleton\n",
        "\n",
        "This colab provides the model code for [MetNet-2](https://ai.googleblog.com/2021/11/metnet-2-deep-learning-for-12-hour.html) as well as general code for preprocessing data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Ruv8nb6E3rho"
      },
      "outputs": [],
      "source": [
        "#@title Install Packages\n",
        "!pip install flax\n",
        "!pip install ml_collections"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "TbD-KaO43tUf"
      },
      "outputs": [],
      "source": [
        "#@title Imports\n",
        "\n",
        "from typing import Optional, Tuple, Callable, Any, Iterable, Union, Dict\n",
        "Array = Any\n",
        "PRNGKey = Any\n",
        "Shape = Iterable[int]\n",
        "Dtype = Any\n",
        "ModuleDef = Any\n",
        "\n",
        "import datetime\n",
        "import functools\n",
        "\n",
        "from flax import linen as nn\n",
        "from flax.linen import initializers\n",
        "from flax.training import common_utils\n",
        "import jax\n",
        "from jax import lax\n",
        "import jax.numpy as jnp\n",
        "import numpy as onp\n",
        "import scipy\n",
        "import ml_collections"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "jqyCUD2X3vBl"
      },
      "outputs": [],
      "source": [
        "#@title Auxiliary Layers and Functions\n",
        "\n",
        "def flatten_spatial_dim(image, factor):\n",
        "  h, w, c = image.shape[-3:]\n",
        "  rest = image.shape[:-3]\n",
        "  f = factor\n",
        "  return image.reshape(rest + (h // f, f, w // f, f, c))\n",
        "\n",
        "\n",
        "def downsample_nanmean(image, factor):\n",
        "  return onp.nanmean(flatten_spatial_dim(image, factor), axis=(-4, -2))\n",
        "\n",
        "\n",
        "def mrms_normalize(a):\n",
        "  \"\"\"Normalize according to\n",
        "     [NaN, inf, -inf, -50, -.5, 0., .2, 1., 2., 10.])\n",
        "     -\u003e\n",
        "     [-1, -1, -1, -1, -1, 0, .046, .172, .268, .537]\n",
        "  \"\"\"\n",
        "  a = onp.nan_to_num(onp.where(a \u003c 0, 0, a + 1), posinf=0, neginf=0)\n",
        "  return onp.tanh(onp.log(a) / 4)\n",
        "\n",
        "def onehot_range(labels, num_classes, on_value=1.0, off_value=0.0):\n",
        "  \"\"\"Onehot but instead of a single 1, multiple 1's in a range is returned.\n",
        "\n",
        "  Similar to common_utils.onehot but instead of 0...010...0, it returns\n",
        "  0...01...10...0 where the provided range is inclusive for both beginning and\n",
        "  end.\n",
        "\n",
        "  Args:\n",
        "    labels: ndarray-like, shape=(..., 2)\n",
        "    num_classes: Number of classes.\n",
        "    on_value: The value to use in the range.\n",
        "    off_value: The value to use outside the range.\n",
        "\n",
        "  Returns:\n",
        "    ndarray-like, shape=(..., num_classes)\n",
        "  \"\"\"\n",
        "  x0 = (labels[..., 0, None] \u003c= jnp.arange(num_classes)[None])\n",
        "  x1 = (labels[..., 1, None] \u003e= jnp.arange(num_classes)[None])\n",
        "  x = x0 \u0026 x1\n",
        "  x = lax.select(x, jnp.full(x.shape, on_value), jnp.full(x.shape, off_value))\n",
        "  return x.astype(jnp.float32)\n",
        "\n",
        "\n",
        "DENSE_INIT = initializers.variance_scaling(\n",
        "    scale=1.0 / 3, mode='fan_out', distribution='uniform')\n",
        "\n",
        "\n",
        "def cond_func(x, cond_input, name):\n",
        "  \"\"\"Condition x on cond_input.\"\"\"\n",
        "  if cond_input is None:\n",
        "    return x\n",
        "  embedding = nn.Dense(\n",
        "      features=2 * x.shape[-1],\n",
        "      use_bias=False,\n",
        "      kernel_init=DENSE_INIT,\n",
        "      name=name)(cond_input)\n",
        "  scale, bias = jnp.split(embedding, 2, axis=-1)\n",
        "  x += bias\n",
        "  x *= scale\n",
        "  return x\n",
        "\n",
        "\n",
        "class ConvLSTMCell(nn.Module):\n",
        "  \"\"\"Convolutional LSTM cell.\"\"\"\n",
        "  kernel_size: Tuple[int, ...]\n",
        "  gate_fn: Callable[[Array], Array] = nn.sigmoid\n",
        "  activation_fn: Callable[[Array], Array] = nn.activation.tanh\n",
        "  kernel_init: Callable[[PRNGKey, Shape, Dtype],\n",
        "                        Array] = initializers.lecun_normal()\n",
        "  recurrent_kernel_init: Callable[[PRNGKey, Shape, Dtype],\n",
        "                                  Array] = initializers.orthogonal()\n",
        "  bias_init: Callable[[PRNGKey, Shape, Dtype], Array] = nn.initializers.zeros\n",
        "  precomputed_inputs: bool = False\n",
        "  dtype: Dtype = jnp.float32\n",
        "\n",
        "  @functools.partial(\n",
        "      nn.scan,\n",
        "      variable_broadcast='params',\n",
        "      in_axes=1,\n",
        "      out_axes=1,\n",
        "      split_rngs={'params': False})\n",
        "  @nn.compact\n",
        "  def __call__(self, carry, inputs):\n",
        "    \"\"\"LSTM cell but with conv projections from the input and hidden state.\"\"\"\n",
        "    assert self.kernel_size is not None\n",
        "    c, h = carry\n",
        "    hidden_features = h.shape[-1]\n",
        "\n",
        "    # input and recurrent layers are summed so only one needs a bias.\n",
        "    dense_h = nn.Conv(\n",
        "        features=4 * hidden_features,\n",
        "        use_bias=True,\n",
        "        kernel_size=self.kernel_size,\n",
        "        kernel_init=self.recurrent_kernel_init,\n",
        "        bias_init=self.bias_init,\n",
        "        padding='SAME',\n",
        "        dtype=self.dtype,\n",
        "        name='h_all')\n",
        "\n",
        "    dense_i = nn.Conv(\n",
        "        features=4 * hidden_features,\n",
        "        use_bias=False,\n",
        "        padding='SAME',\n",
        "        kernel_init=self.kernel_init,\n",
        "        kernel_size=self.kernel_size,\n",
        "        dtype=self.dtype,\n",
        "        name='i_all')\n",
        "\n",
        "    res = dense_h(h)\n",
        "    h_i, h_f, h_g, h_o = jnp.split(res, 4, axis=3)\n",
        "\n",
        "    if self.precomputed_inputs:\n",
        "      i_i, i_f, i_g, i_o = jnp.split(inputs, 4, axis=3)\n",
        "    else:\n",
        "      res = dense_i(inputs)\n",
        "      i_i, i_f, i_g, i_o = jnp.split(res, 4, axis=3)\n",
        "\n",
        "    i = self.gate_fn(i_i + h_i)\n",
        "    f = self.gate_fn(i_f + h_f)\n",
        "    g = self.activation_fn(i_g + h_g)\n",
        "    o = self.gate_fn(i_o + h_o)\n",
        "    new_c = f * c + i * g\n",
        "    new_h = o * self.activation_fn(new_c)\n",
        "    return (new_c, new_h), new_h\n",
        "\n",
        "  @staticmethod\n",
        "  def initialize_carry(batch_dims, hidden_size, init_fn=initializers.zeros):\n",
        "    # use dummy key since default state init fn is just zeros.\n",
        "    return nn.LSTMCell.initialize_carry(\n",
        "        jax.random.PRNGKey(0), batch_dims, hidden_size, init_fn=init_fn)\n",
        "\n",
        "\n",
        "class ResidualBlock(nn.Module):\n",
        "  \"\"\"Bottleneck ResNet block.\"\"\"\n",
        "  filters: int\n",
        "  rezero: bool = False\n",
        "  dtype: Dtype = jnp.float32\n",
        "  groupnorm: Optional[int] = None\n",
        "  activation: Callable[[Array], Array] = nn.relu\n",
        "  kernel_dilation: int = 1\n",
        "  half_channels: bool = False\n",
        "  cond_input: Optional[Array] = None\n",
        "  channel_dropout_rate: Optional[float] = None\n",
        "  bias_scale: Optional[float] = False\n",
        "  train: Optional[bool] = None\n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(self, x):\n",
        "    assert not self.channel_dropout_rate\n",
        "    assert not self.train\n",
        "    assert not self.bias_scale\n",
        "\n",
        "    needs_projection = x.shape[-1] != self.filters\n",
        "    conv = functools.partial(\n",
        "        nn.Conv,\n",
        "        use_bias=False,\n",
        "        dtype=self.dtype,\n",
        "        kernel_dilation=(self.kernel_dilation, self.kernel_dilation))\n",
        "\n",
        "    norm_module = functools.partial(\n",
        "        nn.GroupNorm, num_groups=self.groupnorm, dtype=self.dtype)\n",
        "    norm = lambda x: norm_module()(x) if self.groupnorm else x\n",
        "\n",
        "    r = x\n",
        "    if needs_projection:\n",
        "      r = conv(self.filters, (1, 1), name='proj_conv')(r)\n",
        "\n",
        "    if self.half_channels:\n",
        "      y = conv(self.filters // 2, (3, 3), name='conv1')(x)\n",
        "    else:\n",
        "      y = conv(self.filters, (3, 3), name='conv1')(x)\n",
        "    y = norm(y)\n",
        "    y = cond_func(y, self.cond_input, name='embed1')\n",
        "    y = self.activation(y)\n",
        "\n",
        "    y = conv(self.filters, (3, 3), name='conv2')(y)\n",
        "    if self.rezero:\n",
        "      y = y * self.param('alpha', nn.initializers.zeros, (1,), self.dtype)\n",
        "    y = norm(y)\n",
        "    y = cond_func(y, self.cond_input, name='embed2')\n",
        "\n",
        "    # the skip connection is added in such a way to be compatible with\n",
        "    # trained checkpoints.\n",
        "    if self.cond_input is None:\n",
        "      y = self.activation(y + r)\n",
        "    else:\n",
        "      y = self.activation(y)\n",
        "      y += r\n",
        "    return y\n",
        "\n",
        "\n",
        "class ResidualStack(nn.Module):\n",
        "  \"\"\"Stack of residual modules.\"\"\"\n",
        "  num_blocks: int\n",
        "  filters: int\n",
        "  groupnorm: Optional[int] = None\n",
        "  kernel_dilations: Tuple[int, ...] = (1, 2, 4, 8, 16, 32)\n",
        "  dtype: Dtype = jnp.float32\n",
        "  # Arguments to residual block constructor.\n",
        "  half_channels: Optional[bool] = None\n",
        "  cond_input: Optional[Array] = None\n",
        "  channel_dropout_rate: Optional[float] = None\n",
        "  bias_scale: Optional[bool] = False\n",
        "  train: Optional[bool] = None\n",
        "  extra_resnet_block_kwargs: Optional[Dict[str, Any]] = None\n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(self, x):\n",
        "    \"\"\"Applies a stack of residual modules.\n",
        "\n",
        "    Args:\n",
        "      x: The input array.\n",
        "\n",
        "    Returns:\n",
        "      The result of applying the residual stack and passing it through a relu.\n",
        "    \"\"\"\n",
        "    dense = functools.partial(nn.Dense, features=self.filters, dtype=self.dtype)\n",
        "    residual_block = functools.partial(\n",
        "        ResidualBlock,\n",
        "        filters=self.filters,\n",
        "        groupnorm=self.groupnorm,\n",
        "        dtype=self.dtype,\n",
        "        half_channels=self.half_channels,\n",
        "        cond_input=self.cond_input,\n",
        "        channel_dropout_rate=self.channel_dropout_rate,\n",
        "        bias_scale=self.bias_scale,\n",
        "        train=self.train,\n",
        "        **(self.extra_resnet_block_kwargs or {}))\n",
        "    x_tot = dense(name='skip_init')(x)\n",
        "\n",
        "    for i in range(self.num_blocks):\n",
        "      dilation = self.kernel_dilations[i % len(self.kernel_dilations)]\n",
        "      x = residual_block(name=f'block{i}', kernel_dilation=dilation)(x)\n",
        "      x_tot += dense(name=f'skip{i}')(x)\n",
        "\n",
        "    return nn.relu(x_tot)\n",
        "\n",
        "\n",
        "class DenseStack(nn.Module):\n",
        "  \"\"\"A stack of dense layers.\"\"\"\n",
        "  features: int = 4096\n",
        "  output_channels: int = 512\n",
        "  num_layers: int = 1\n",
        "  dtype: Dtype = jnp.float32\n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(self, inputs):\n",
        "    out = inputs\n",
        "\n",
        "    for _ in range(self.num_layers):\n",
        "      out = nn.Dense(\n",
        "          features=self.features,\n",
        "          use_bias=False,\n",
        "          kernel_init=DENSE_INIT,\n",
        "          dtype=self.dtype)(out)\n",
        "      out = nn.relu(out)\n",
        "\n",
        "    pre_out = out\n",
        "    out = nn.Dense(\n",
        "        features=self.output_channels,\n",
        "        use_bias=False,\n",
        "        kernel_init=DENSE_INIT,\n",
        "        dtype=self.dtype)(pre_out)\n",
        "    return out, pre_out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "NoZoi0xU3whR"
      },
      "outputs": [],
      "source": [
        "#@title Main Model\n",
        "\n",
        "def metnet_encoder(inputs,\n",
        "                   target_index,\n",
        "                   model_target_index,\n",
        "                   hps,\n",
        "                   is_initializing):\n",
        "  \"\"\"MetNet encoder architecture.\"\"\"\n",
        "  lstm_channels = hps.lstm_channels\n",
        "  encoder_channels = hps.encoder_channels\n",
        "  encoder_num_blocks = hps.encoder_num_blocks\n",
        "  num_time_classes = len(hps.mrms_target_tds)\n",
        "\n",
        "  if hps.shift_target_index:\n",
        "    # For predicting cumulative, we use an alternative one hot encoding which\n",
        "    # has 30 ones instead of a single one, each representing 2 min. I.e. a full\n",
        "    # 30 ones is the prediction of a full cumulative hour. By adding 29 to the\n",
        "    # target index, there will also be 30 ones for predicting 2 min to 58 min.\n",
        "    # This is especially important for our mixed models so the model can\n",
        "    # distinquish between predicting rate and cumulative precipitation.\n",
        "    num_time_classes = num_time_classes + 29\n",
        "    model_target_index = model_target_index + 29\n",
        "\n",
        "  # configurations for conditioning target index.\n",
        "  target_cond = hps.target_cond\n",
        "  target_features = hps.target_features\n",
        "  target_n_layers = hps.target_n_layers\n",
        "  cond_per_layer = hps.cond_per_layer\n",
        "\n",
        "  groupnorm = hps.groupnorm\n",
        "  target_size = hps.target_size\n",
        "\n",
        "  dtype = jnp.bfloat16 if hps.dtype == 'bfloat16' else jnp.float32\n",
        "\n",
        "  # Remove disabled inputs (constant zeros).\n",
        "  # Flax (non-Linen) doesn't support None inputs.\n",
        "  inputs = [input_ for input_ in inputs\n",
        "            if input_ is not None and input_.ndim != 1]\n",
        "\n",
        "  batch_size = target_index.shape[0]\n",
        "  num_steps = max([input_.shape[1] for input_ in inputs])\n",
        "  input_size = onp.unique([input_.shape[2] for input_ in inputs]).item()\n",
        "\n",
        "  def target_index_block(model_target_index):\n",
        "    target_index = onehot_range(\n",
        "        model_target_index, num_classes=num_time_classes)\n",
        "    target_index = jnp.reshape(target_index, (-1, 1, 1, 1, num_time_classes))\n",
        "    return jnp.broadcast_to(\n",
        "        target_index,\n",
        "        (batch_size, num_steps, input_size, input_size, num_time_classes))\n",
        "\n",
        "  def concatenate_block(*inputs):\n",
        "    padded_inputs = []\n",
        "    for input_ in inputs:\n",
        "      pad_width = [(0, 0), (num_steps - input_.shape[1], 0), (0, 0), (0, 0),\n",
        "                   (0, 0)]\n",
        "      padded_inputs.append(jnp.pad(input_, pad_width, mode='constant'))\n",
        "    return jnp.concatenate(padded_inputs, axis=-1)\n",
        "\n",
        "  assert target_cond in ['concat', 'dense_scale', 'dense_bias']\n",
        "\n",
        "  # For the paper (see appendix) we tried multiple types of conditioning.\n",
        "  # `dense_scale` is what ended up being used in the paper.\n",
        "  # `dense_bias` is \"Add, No Mult\" in the paper.\n",
        "  # `concat` is \"No Add, No Mult\" in the paper.\n",
        "  if target_cond == 'concat':\n",
        "    target_index = target_index_block(model_target_index)\n",
        "    inputs = concatenate_block(*(inputs + [target_index]))\n",
        "    target_embed = target_index\n",
        "  elif 'dense' in target_cond:\n",
        "    inputs = concatenate_block(*inputs)\n",
        "    target_index = onehot_range(\n",
        "        model_target_index, num_classes=num_time_classes)\n",
        "    target_index = jnp.reshape(target_index, (-1, 1, 1, 1, num_time_classes))\n",
        "\n",
        "    output_channels = inputs.shape[-1]\n",
        "    if target_cond == 'dense_scale':\n",
        "      output_channels = 2 * inputs.shape[-1]\n",
        "    out, target_embed = DenseStack(\n",
        "        output_channels=output_channels,\n",
        "        features=target_features,\n",
        "        num_layers=target_n_layers)(target_index)\n",
        "    if target_cond == 'dense_scale':\n",
        "      target_bias, target_scale = jnp.split(out, 2, axis=-1)\n",
        "      inputs += target_bias\n",
        "      inputs *= target_scale\n",
        "    elif target_cond == 'dense_bias':\n",
        "      inputs += out\n",
        "    target_embed = jnp.squeeze(target_embed, axis=1)\n",
        "\n",
        "  # Layers definition\n",
        "  init_carry = ConvLSTMCell.initialize_carry(\n",
        "      (batch_size, input_size, input_size), lstm_channels)\n",
        "  lstm_cell = ConvLSTMCell(\n",
        "      kernel_size=(3, 3), dtype=dtype, name='conv_lstm0')\n",
        "\n",
        "  def make_encoder(i):\n",
        "    return ResidualStack(\n",
        "        num_blocks=encoder_num_blocks[i],\n",
        "        filters=encoder_channels[i],\n",
        "        kernel_dilations=(1, 2, 4, 8, 16, 32, 64, 128),\n",
        "        half_channels=False,\n",
        "        groupnorm=groupnorm,\n",
        "        cond_input=target_embed if cond_per_layer else None,\n",
        "        name=f'encoder{i}')\n",
        "\n",
        "  def crop_to_target(x, target_size, downsampling_ratio):\n",
        "    ds_ts = target_size // downsampling_ratio\n",
        "    x_start = (x.shape[1] - ds_ts) // 2\n",
        "    return x[:, x_start:x_start + ds_ts, x_start:x_start + ds_ts, :]\n",
        "\n",
        "  def upsample_by_repeat(x, times):\n",
        "    # X = [B, H, W, F]\n",
        "    b, h, w, f = x.shape\n",
        "    x = x.reshape((b, h, 1, w, 1, f))\n",
        "    x = jnp.broadcast_to(x, (b, h, times, w, times, f))\n",
        "    x = x.reshape((b, h * times, w * times, f))\n",
        "    return x\n",
        "\n",
        "  carry, _ = lstm_cell(init_carry, inputs)\n",
        "  time_encoded_input = jnp.concatenate(carry, axis=-1)\n",
        "\n",
        "  # First stage\n",
        "  encoded_input = make_encoder(0)(time_encoded_input)\n",
        "\n",
        "  # Second stage after first crop\n",
        "  start_crop = input_size // 4\n",
        "  size_crop = input_size // 2\n",
        "  encoded_input = encoded_input[:, start_crop:start_crop + size_crop,\n",
        "                                start_crop:start_crop + size_crop, :]\n",
        "\n",
        "  if encoder_num_blocks[1] \u003e 0:\n",
        "    encoded_input = make_encoder(1)(encoded_input)\n",
        "\n",
        "  resolution = 4\n",
        "  encoded_input = crop_to_target(encoded_input, target_size, resolution)\n",
        "  repeated_encoded_input = upsample_by_repeat(encoded_input, resolution)\n",
        "\n",
        "  return repeated_encoded_input, target_embed\n",
        "\n",
        "class MetNet2(nn.Module):\n",
        "  \"\"\"MetNet2\"\"\"\n",
        "  input_keys = [\n",
        "      'mrms',\n",
        "      'mrms_cumulative',\n",
        "      'goes',\n",
        "      'hrrr',\n",
        "      'target_index',\n",
        "      'model_target_index',\n",
        "  ]\n",
        "  num_output_channels: int = 512\n",
        "  hps: Optional[ml_collections.ConfigDict] = None\n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(self, mrms, mrms_cumulative, goes, hrrr,\n",
        "               target_index, model_target_index, train):\n",
        "    hps = self.hps\n",
        "    # Detect if we're initializing by absence of params.\n",
        "    is_initializing = not self.has_variable('params', 'upsampler')\n",
        "\n",
        "    upsampler_channels = hps.upsampler_channels\n",
        "    upsampler_num_blocks = hps.upsampler_num_blocks\n",
        "    upsampler_cond_per_layer = hps.upsampler_cond_per_layer\n",
        "    groupnorm = hps.groupnorm\n",
        "\n",
        "    dtype = jnp.bfloat16 if hps.dtype == 'bfloat16' else jnp.float32\n",
        "\n",
        "    inputs = [mrms, mrms_cumulative, goes, hrrr]\n",
        "    print(f'Dimensions before encoding: {[a.shape for a in inputs]}')\n",
        "    repeated_encoded_input, target_embed = metnet_encoder(\n",
        "        inputs, target_index,\n",
        "        model_target_index, hps,\n",
        "        is_initializing)\n",
        "    print(f'Dimensions after encoding: {repeated_encoded_input.shape}')\n",
        "\n",
        "    def final_block_fn(hidden_size, prefix):\n",
        "      def compute_outputs(input_):\n",
        "        pre_final_dense = nn.Dense(\n",
        "            features=hidden_size, name=f'{prefix}prefinal', dtype=dtype)\n",
        "\n",
        "        final_dense = nn.Dense(\n",
        "            features=self.num_output_channels,\n",
        "            name=f'{prefix}final',\n",
        "            dtype=jnp.float32)\n",
        "\n",
        "        logits = final_dense(nn.relu(pre_final_dense(input_)))\n",
        "\n",
        "        return logits\n",
        "      return compute_outputs\n",
        "\n",
        "    # Upsampler stage\n",
        "\n",
        "    upsampler = ResidualStack(\n",
        "        num_blocks=upsampler_num_blocks,\n",
        "        kernel_dilations=(1,),\n",
        "        filters=upsampler_channels,\n",
        "        half_channels=False,\n",
        "        cond_input=target_embed if upsampler_cond_per_layer else None,\n",
        "        groupnorm=groupnorm,\n",
        "        name='upsampler')\n",
        "\n",
        "    def upsampler_block(all_encoded_inputs):\n",
        "      # Upsample and combine\n",
        "      return upsampler(all_encoded_inputs)\n",
        "\n",
        "\n",
        "    out = upsampler_block(repeated_encoded_input)\n",
        "\n",
        "    outputs = final_block_fn(\n",
        "        hidden_size=hps.pre_final_size_1km_resolution,\n",
        "        prefix='')(out)\n",
        "\n",
        "    print(f'Final output shape: {outputs.shape}')\n",
        "\n",
        "    return outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "ffCXsn8M3yJT"
      },
      "outputs": [],
      "source": [
        "#@title Hyperparameters/config\n",
        "\n",
        "hps = ml_collections.ConfigDict()\n",
        "hps.dtype = 'bfloat16'\n",
        "\n",
        "hps.mrms_target_tds = list(range(2, 12 * 60 + 1, 2))\n",
        "hps.bins = onp.linspace(0, 102.4, 513, dtype=onp.float32)\n",
        "\n",
        "# Model parameters.\n",
        "hps.lstm_channels = 128\n",
        "hps.encoder_channels = [384, 384]\n",
        "hps.encoder_num_blocks = [16, 8]\n",
        "hps.upsampler_channels = 512\n",
        "hps.upsampler_num_blocks = 2\n",
        "hps.remat = False\n",
        "hps.pre_final_size_1km_resolution = 4096\n",
        "\n",
        "hps.shift_target_index = True\n",
        "hps.target_cond = 'dense_scale'\n",
        "hps.target_n_layers = 2\n",
        "hps.target_features = 2048\n",
        "hps.target_size = 512\n",
        "hps.cond_per_layer = True\n",
        "hps.upsampler_cond_per_layer = True\n",
        "hps.groupnorm = None\n",
        "\n",
        "# Training parameters.\n",
        "hps.train_steps = 500000\n",
        "hps.batch_size = 16\n",
        "hps.sampling_priority_exp = 2\n",
        "hps.train_mask = 'roi'\n",
        "hps.learning_rate = 2e-5\n",
        "hps.optimizer = 'adam'\n",
        "hps.lr_schedule = 'none'\n",
        "hps.optimizer_beta = 0.9\n",
        "hps.optimizer_decay_steps = '100000'\n",
        "hps.polyak_decay = 0.9999\n",
        "hps.weight_decay = 1e-1\n",
        "\n",
        "hps = ml_collections.FrozenConfigDict(hps)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "mpNTZ-FK3z-Y"
      },
      "outputs": [],
      "source": [
        "#@title Input and Preprocessing\n",
        "\n",
        "# All the spatial inputs are expected to be on a evenly spaced 1km grid.\n",
        "\n",
        "batch_size = 1 # @param {type: 'integer'}\n",
        "spatial_dim = 512 # @param {type: 'integer'}\n",
        "#@markdown ##### in km. NOTE: 2048 for the full mode, 512 is for testing\n",
        "\n",
        "# Time of the sample.\n",
        "sample_time = [datetime.datetime(2020, 3, 1, 12, 0, 0)] * batch_size\n",
        "\n",
        "# Timedelta to predict.\n",
        "td = onp.array([60] * batch_size)  # In minutes\n",
        "\n",
        "# Elevation.\n",
        "# NOTE: Should be replaced with real data\n",
        "elevation = onp.zeros((batch_size, 1, spatial_dim, spatial_dim, 1), jnp.float32)\n",
        "\n",
        "# Longitude/latitude.\n",
        "# NOTE: Should be replaced with real data\n",
        "lon_lat = onp.zeros((batch_size, 1, spatial_dim, spatial_dim, 2), jnp.float32)\n",
        "\n",
        "# MRMS precipitation rate input with 11 time slices.\n",
        "# [-90 min, -75 min, -60 min, -45 min, -30 min, -25 min, -20 min, -15 min, -10 min, -5 min, 0 min]\n",
        "# NOTE: Should be replaced with real data\n",
        "mrms_rate = onp.zeros((batch_size, 11, spatial_dim, spatial_dim, 1), onp.float32)\n",
        "\n",
        "# MRMS cumulative precipitation input with 2 time slices.\n",
        "# [-60 min, 0 min]\n",
        "# NOTE: Should be replaced with real data\n",
        "mrms_cumulative = onp.zeros((batch_size, 2, spatial_dim, spatial_dim, 1), jnp.float32)\n",
        "\n",
        "# GOES satellite input with 3 time slices.\n",
        "# [-30 min, -15 min, 0 min]\n",
        "# NOTE: Should be replaced with real data\n",
        "goes = onp.zeros((batch_size, 3, spatial_dim, spatial_dim, 16), jnp.float32)\n",
        "\n",
        "# HRRR assimilated input with 2 time slices.\n",
        "# [-60 min, 0 min]\n",
        "# NOTE: Should be replaced with real data\n",
        "hrrr = onp.zeros((batch_size, 3, spatial_dim, spatial_dim, 612), jnp.float32)\n",
        "\n",
        "# Example of a target with precipitation.\n",
        "target_precipitation = onp.zeros((batch_size, spatial_dim, spatial_dim, 1), jnp.float32)\n",
        "\n",
        "print(f'Sample for {td} min prediction from time {sample_time}')\n",
        "print()\n",
        "print('Raw sample dimensions:')\n",
        "print(f'Elevation: {elevation.shape}')\n",
        "print(f'Longitude/lattitude: {lon_lat.shape}')\n",
        "print(f'MRMS precipitation rate: {mrms_rate.shape}')\n",
        "print(f'MRMS cumulative rate: {mrms_cumulative.shape}')\n",
        "print(f'GOES: {goes.shape}')\n",
        "print(f'HRRR: {hrrr.shape}')\n",
        "print(f'Target: {target_precipitation.shape}')\n",
        "\n",
        "\n",
        "# Preprocessing the data.\n",
        "elevation = elevation / 2000.\n",
        "\n",
        "lon_lat = lon_lat / 1000.\n",
        "\n",
        "sample_time = onp.array([[x.hour, x.day, x.month] for x in sample_time],\n",
        "                        dtype=jnp.bfloat16)\n",
        "sample_time = onp.reshape(sample_time, [batch_size, 1, 1, 1, 3])\n",
        "sample_time = onp.tile(sample_time, [1, 1, spatial_dim, spatial_dim, 1])\n",
        "\n",
        "rest = onp.concatenate([elevation, lon_lat, sample_time], axis=-1)\n",
        "rest = downsample_nanmean(rest, 4)\n",
        "\n",
        "mrms_rate = downsample_nanmean(mrms_rate, 4)\n",
        "mrms_rate = mrms_normalize(mrms_rate)\n",
        "mrms_rate = onp.concatenate([\n",
        "    mrms_rate,\n",
        "    onp.tile(rest, [1, mrms_rate.shape[1], 1, 1, 1])  # To fit the time dimension.\n",
        "], axis=-1)\n",
        "mrms_rate = mrms_rate.astype(jnp.bfloat16)\n",
        "\n",
        "mrms_cumulative = downsample_nanmean(mrms_cumulative, 4)\n",
        "mrms_cumulative = mrms_normalize(mrms_cumulative)\n",
        "mrms_cumulative = onp.concatenate([\n",
        "    mrms_cumulative,\n",
        "    onp.tile(rest, [1, mrms_cumulative.shape[1], 1, 1, 1])  # To fit the time dimension.\n",
        "], axis=-1)\n",
        "mrms_cumulative = mrms_cumulative.astype(jnp.bfloat16)\n",
        "\n",
        "goes = downsample_nanmean(goes, 4)\n",
        "goes = goes # Data should be standardized\n",
        "goes = onp.concatenate([\n",
        "    goes,\n",
        "    onp.tile(rest, [1, goes.shape[1], 1, 1, 1])  # To fit the time dimension.\n",
        "], axis=-1)\n",
        "goes = goes.astype(jnp.bfloat16)\n",
        "\n",
        "hrrr = downsample_nanmean(hrrr, 4)\n",
        "hrrr = hrrr # Data should be standardized\n",
        "hrrr = onp.concatenate([\n",
        "    hrrr,\n",
        "    onp.tile(rest, [1, hrrr.shape[1], 1, 1, 1])  # To fit the time dimension.\n",
        "], axis=-1)\n",
        "hrrr = hrrr.astype(jnp.bfloat16)\n",
        "\n",
        "# Target needs to be logits based on one hot encoding of the precipitation bins.\n",
        "target_precipitation = onp.clip(\n",
        "    onp.digitize(target_precipitation[..., 0], hps.bins) - 1, 0, len(hps.bins) - 2)\n",
        "target_precipitation = common_utils.onehot(\n",
        "    target_precipitation, len(hps.bins) - 1)\n",
        "target_precipitation = onp.log(target_precipitation)\n",
        "\n",
        "# Target index. The time delta the model has to predict.\n",
        "is_cumulative = False  # True, if the model should predict 60 min cumulative precipitation.\n",
        "target_index = td // 2 - 1\n",
        "\n",
        "if is_cumulative:\n",
        "  # For 1hr cumulative precipitation the ones gets a range of 30 1's as input\n",
        "  # instead of a single 1 (onehot encoding) to distinquich between the two.\n",
        "  model_target_index = onp.stack([target_index - 29, target_index], axis=-1)\n",
        "else:\n",
        "  model_target_index = onp.stack([target_index, target_index], axis=-1)\n",
        "\n",
        "target_index = target_index.astype(jnp.bfloat16)\n",
        "model_target_index = model_target_index.astype(jnp.bfloat16)\n",
        "\n",
        "print()\n",
        "print('Preprocessed sample dimensions:')\n",
        "print(f'MRMS precipitation rate: {mrms_rate.shape}')\n",
        "print(f'MRMS cumulative rate: {mrms_cumulative.shape}')\n",
        "print(f'GOES: {goes.shape}')\n",
        "print(f'HRRR: {hrrr.shape}')\n",
        "print(f'Target: {target_precipitation.shape}')\n",
        "\n",
        "inputs = {\n",
        "    'mrms': mrms_rate,\n",
        "    'mrms_cumulative': mrms_cumulative,\n",
        "    'goes': goes,\n",
        "    'hrrr': hrrr,\n",
        "    'target_index': target_index,\n",
        "    'model_target_index': model_target_index,\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "qL_VbWHK32MZ"
      },
      "outputs": [],
      "source": [
        "#@title Inference\n",
        "\n",
        "num_output_channels = len(hps.bins) - 1\n",
        "\n",
        "module = MetNet2(hps=hps, num_output_channels=num_output_channels)\n",
        "\n",
        "jit_init = jax.jit(module.init, static_argnames='train')\n",
        "rng = jax.random.PRNGKey(0)\n",
        "variables = jit_init(rng, **inputs, train=False)\n",
        "\n",
        "params = variables['params']\n",
        "apply = jax.jit(module.apply, static_argnames='train')\n",
        "\n",
        "num_params = sum([x.size for x in jax.tree_util.tree_leaves(params)])\n",
        "print(f'Number of trainable parameters: {num_params}')\n",
        "\n",
        "result = apply({'params': params}, **inputs, train=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "pJsTblRF3380"
      },
      "outputs": [],
      "source": [
        "#@title Probabilities from output\n",
        "\n",
        "# Minimum milimeter of precpitation one wants probability of.\n",
        "mm = 2.\n",
        "\n",
        "i, = onp.where(hps.bins == mm)\n",
        "i = i.item()\n",
        "\n",
        "probs = nn.softmax(result, axis=-1)\n",
        "mm_probs = jnp.cumsum(probs[..., ::-1], axis=-1)[..., ::-1][..., i]\n",
        "\n",
        "print(f'Shape of probabilities for at least {mm} mm of precipitation rate/1hr accumulated: {mm_probs.shape}')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "last_runtime": {
        "build_target": "//learning/brain/research/dune/experimental/brainstorm:e2ew_colab",
        "kind": "private"
      },
      "name": "colab.ipynb",
      "private_outputs": true,
      "provenance": [
        {
          "file_id": "1vAvB7mU29tgoLvJzljKqQ06v2x6X1wAa",
          "timestamp": 1658912030956
        }
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
